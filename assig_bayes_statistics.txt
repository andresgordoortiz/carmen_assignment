Prof. Carles Sánchez
Departament de Fı́sica, UAB, & IFAE
carles.sanchez@uab.cat
Statistics and Data Analysis (2025/26)
Assignment on Bayesian Statistics
Instructions
1. The assignment has to be done as a jupyter python3 notebook, which should be uploaded to
Classroom along with the PDF of the same notebook.
2. The deadline for the assignment is Nov 7, 2025.
3. The notebook has to be commented and all steps explained/justified, similar to the notebooks
we have seen during the classes.
Assignment
A cosmological experiment1 provides a measurement of the expansion history of the Universe,
H(z), obtained from the ages of passively-evolving galaxies in galaxy clusters at various redshifts,
z. These measurements of H(z) can be used to constrain the Hubble parameter H0 , quantifying
the local expansion rate, and the matter density in the Universe, Ωm , using the following model:
q
H(z) = H0 Ωm (1 + z)3 + (1 − Ωm ).
(1)
The measurements can be read from the data file in this assignment using this python code:
1
2
import numpy as np
z, h, herr = np.loadtxt('Hz_BC03_all.dat',unpack=True)
If D denotes the data provided by the measurements of H(z), and H0 , Ωm are the parameters of
the model of Equation (1), we want to use Bayesian parameter inference to compute the posterior
probability
p(H0 , Ωm |D) ∝ p(D|H0 , Ωm ) p(H0 , Ωm )
using Monte Carlo Markov Chains (with the Metropolis implementation). For the likelihood,
p(D|H0 , Ωm ), you can assume a Gaussian likelihood, given the measurement uncertainties that are
provided in the data file (the column named herr above). In the case of this example, the data is
composed of Hi measurements at different zi , with uncertainties σi . The Gaussian likelihood will
then be computed as:
p(D|H0 , Ωm ) =
Y
p(Di |H0 , Ωm ) =
i
Y
exp −(Hi − H(zi , H0 , Ωm ))2 /σi2 ,
i
where H(zi , H0 , Ωm ) is the evaluation of the model at zi with parameters H0 , Ωm . This can also
be written as:
p(D|H0 , Ωm ) = exp −
1
https://arxiv.org/abs/0907.3149
X
χ2
, with χ2 =
(Hi − H(zi , H0 , Ωm ))2 /σi2
2
iAssignment on Bayesian Statistics
Statistics and Data Analysis (2025/26)
To successfully complete this assignment, please follow the steps below:
1. Plot the measurements with their error bars along a handful of parameter combinations for
the H(z) model, some similar to the data, some different.
2. Define python functions for the model, the posterior (likelihood times prior) and the proposal
distribution. For the prior distribution on the parameters, you can start by using a uniform
prior over 50 < H0 < 100 and 0 < Ωm < 1.
3. Write your MCMC code and run 4 or 5 chains, initializing them at different locations in the
two parameters. Study them, plotting the parameters against the chain step, and cut out the
burn-in regions.
4. Study the efficiency of the chains (steps accepted / steps proposed) and the convergence,
running several independent chains starting from different points and checking that you get
consistent posteriors.
5. From the samples of the chains, make a 2D density plot of H0 , Ωm , and also make 1D plots
of the histogram of each parameter (do they look Gaussian?). Calculate the 68% confidence
interval around the mode of the distribution on each of the two parameters (please do it by
counting on the histogram of the samples and not computing the standard deviation, as it
could be non-symmetric).
6. Repeat the plot of point 1, but now with parameter combinations drawn directly from the
posterior.
7. Finally, repeat the whole exercise but now using a Gaussian prior on Ωm = 0.315 ± 0.007,
given by measurements of the Planck Satellite2 . This is a good example on how to use prior
information in our analysis. What is the new 68% confidence interval on H0 ? Comment on
why is it larger or smaller. And what is the new 68% confidence interval on Ωm , and why?
Please do all the above checks on the sanity of the chains in this case.
Final remark: Please note that, even if this looks like a very specific example, the exact same
procedure could be used on any problem concerning some data and a model with various parameters,
where you want to estimate the parameters based on the data and your prior knowledge about the
parameters. It is likely that all of you will encounter that sort of problem at some point (or often!),
and now you have coded a very elegant and robust way to perform that parameter inference, and
it can be ran in your laptop in just a minute or two. I hope it will be useful for you in the future!
2
https://arxiv.org/abs/1807.06209
2